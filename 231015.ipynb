{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "private_outputs": true,
      "gpuType": "T4",
      "mount_file_id": "1WV3G26pMkVJCxuADVc8EivhCgRoQ0hvn",
      "authorship_tag": "ABX9TyONPnTELrB+sMGoLD6OrmKT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JHyunjun/SuperResolution/blob/main/231015.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Em_0oI8lBPbm"
      },
      "outputs": [],
      "source": [
        "# Created By unknown-user(Hyundai CTO Dacon)\n",
        "# From 23.10.14"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip\n",
        "#%cd /content/drive/MyDrive/Colab_Notebooks/Hyundai_Daycon/train_hr_1\n",
        "#!unzip -qq \"/content/drive/MyDrive/Colab_Notebooks/Hyundai_Daycon/train_hr_1/hr.zip\""
      ],
      "metadata": {
        "id": "g_QC-iHE-4WO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "metadata": {
        "id": "bBMKJTCs_Odl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Salt and pepper Denoising\n",
        "\n",
        "def selective_median_filter(img, kernel_size=3, threshold1 = 0, threshold2 = 1):\n",
        "    # Masks where the image has values close to 0 or 1\n",
        "    mask_salt = img > (1.0 - threshold1) # white\n",
        "    mask_pepper = img < threshold2 # black\n",
        "    mask_combined = tf.math.logical_or(mask_salt, mask_pepper).numpy()\n",
        "\n",
        "    # Convert image to numpy for processing\n",
        "    img_numpy = img.numpy()\n",
        "\n",
        "    # Apply median filter only where the mask is true\n",
        "    for i in range(img_numpy.shape[2]):  # For each channel\n",
        "        channel = img_numpy[:,:,i]\n",
        "        channel_filtered = median_filter(channel, size=kernel_size)\n",
        "        #channel_filtered = uniform_filter(channel, size=kernel_size)\n",
        "        # Replace only the noisy pixels\n",
        "        channel[mask_combined[:,:,0]] = channel_filtered[mask_combined[:,:,0]]\n",
        "        img_numpy[:,:,i] = channel\n",
        "\n",
        "    return tf.convert_to_tensor(img_numpy, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "MyYelg6khuAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Lambda, Input, Add, Dense, Reshape, Flatten, UpSampling2D, Conv2D, LeakyReLU, BatchNormalization, Activation,Conv2DTranspose\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import scipy.ndimage as ndimage\n",
        "from scipy.ndimage import median_filter, uniform_filter\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import random\n",
        "import cv2\n"
      ],
      "metadata": {
        "id": "wAVnWdx8Btz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "directory_path_train_lr = \"/content/drive/MyDrive/Colab_Notebooks/Hyundai_Daycon/train_lr\"\n",
        "directory_path_train_hr = \"/content/drive/MyDrive/Colab_Notebooks/Hyundai_Daycon/train_hr_1\""
      ],
      "metadata": {
        "id": "P6cbMpUNB70x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "image_slicing = 2000\n",
        "\n",
        "lr_image_files = sorted([os.path.join(directory_path_train_lr, f) for f in os.listdir(directory_path_train_lr)])\n",
        "hr_image_files = sorted([os.path.join(directory_path_train_hr, f) for f in os.listdir(directory_path_train_hr)])\n",
        "\n",
        "lr_image_files_3000 = lr_image_files[:image_slicing]\n",
        "hr_image_files_3000 = hr_image_files[:image_slicing]\n",
        "\n",
        "# pre-processing\n",
        "def load_images(lr_path, hr_path):\n",
        "    lr = tf.io.read_file(lr_path)\n",
        "    lr = tf.image.decode_jpeg(lr, channels=3)\n",
        "    lr = tf.image.resize(lr, [256, 256])\n",
        "    lr = lr / 255.0\n",
        "    lr_filter = tf.py_function(func=selective_median_filter, inp=[lr], Tout=tf.float32)\n",
        "\n",
        "    hr = tf.io.read_file(hr_path)\n",
        "    hr = tf.image.decode_jpeg(hr, channels=3)\n",
        "    hr = tf.image.resize(hr, [1024, 1024])\n",
        "    hr = hr / 255.0\n",
        "\n",
        "    return lr_filter, hr\n",
        "    #return lr, hr, lr_filter\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((lr_image_files_3000, hr_image_files_3000))\n",
        "train_dataset = train_dataset.map(load_images)\n",
        "train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "Vvop0mZSGxIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_images(dataset, num_samples=3):\n",
        "    #for i, (lr_image, hr_image, lr_filter) in enumerate(dataset.unbatch().take(num_samples)):\n",
        "    for i, (lr_image, hr_image) in enumerate(dataset.unbatch().take(num_samples)):\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.imshow(lr_image.numpy())\n",
        "        plt.title(f\"Low Resolution Image {i+1}\")\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        #plt.imshow(lr_filter.numpy())\n",
        "        plt.title(f\"Filtering Image {i+1}\")\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.imshow(hr_image.numpy())\n",
        "        plt.title(f\"High Resolution Image {i+1}\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "show_images(train_dataset)"
      ],
      "metadata": {
        "id": "7Yf2JVjqC2Pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Super Resolution\n",
        "\n",
        "def residual_block(x, filters, kernel_size=3, scaling=0.1):\n",
        "    tmp = Conv2D(filters, kernel_size, padding='same', activation='relu')(x)\n",
        "    tmp = Conv2D(filters, kernel_size, padding='same')(tmp)\n",
        "    tmp = Add()([x, tmp])\n",
        "    return tmp\n",
        "\n",
        "def upsample(x, scale=4):\n",
        "    return Lambda(lambda x: tf.nn.depth_to_space(x, scale))(x)\n",
        "\n",
        "def EDSR(input_shape=(256,256,3), scale=4):\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    x = Conv2D(64, 9, padding='same', activation='relu')(inputs)\n",
        "    for _ in range(5):  # Number of residual blocks\n",
        "        x = residual_block(x, 64)\n",
        "\n",
        "    x = Conv2D(64, 3, padding='same')(x)\n",
        "    x = upsample(x, scale)\n",
        "    outputs = Conv2D(3, 9, padding='same')(x)\n",
        "\n",
        "    return Model(inputs, outputs)\n",
        "\n",
        "model = EDSR()\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.summary()\n",
        "\n",
        "model.fit(train_dataset, epochs = 40)"
      ],
      "metadata": {
        "id": "ojDrmHgRH7Px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_images(dataset, num_samples=3):\n",
        "    #for i, (lr_image, hr_image, lr_filter) in enumerate(dataset.unbatch().take(num_samples)):\n",
        "    for i, (lr_image, hr_image) in enumerate(dataset.unbatch().take(num_samples)):\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.imshow(lr_image.numpy())\n",
        "        plt.title(f\"Low Resolution Image {i+1}\")\n",
        "\n",
        "        lr_image = tf.expand_dims(lr_image, 0)\n",
        "        sr = model.predict(lr_image)\n",
        "        sr = np.clip(sr, 0, 1)\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(tf.squeeze(sr))\n",
        "        plt.title(f\"Super Resolution Image {i+1}\")\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.imshow(hr_image.numpy())\n",
        "        plt.title(f\"High Resolution Image {i+1}\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "show_images(train_dataset)"
      ],
      "metadata": {
        "id": "40-BNiwgdVO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testifying\n",
        "\n",
        "directory_path_test_lr = \"/content/drive/MyDrive/Colab_Notebooks/Hyundai_Daycon/test/lr\"\n",
        "test_image_files = sorted([os.path.join(directory_path_test_lr, fname) for fname in os.listdir(directory_path_test_lr) if fname.endswith('.png') or fname.endswith('.jpg')])\n",
        "\n",
        "def load_and_preprocess_image(img_path):\n",
        "    img = load_img(img_path, target_size=(256, 256))\n",
        "    img = img_to_array(img)\n",
        "    img = img / 255.0\n",
        "    img_filter = tf.py_function(func=selective_median_filter, inp=[img], Tout=tf.float32)\n",
        "    return img_filter\n",
        "\n",
        "selected_test_image_files = random.sample(test_image_files, 10)\n",
        "\n",
        "for test_img_path in selected_test_image_files:\n",
        "    test_img = load_and_preprocess_image(test_img_path)\n",
        "    test_img_input = tf.expand_dims(test_img, 0)\n",
        "\n",
        "    sr_img = model.predict(test_img_input)\n",
        "    sr_img = tf.squeeze(sr_img)\n",
        "    sr_img = np.clip(sr_img, 0, 1)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(test_img)\n",
        "    plt.title(\"LR Image\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(sr_img)\n",
        "    plt.title(\"SR Image\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "uOHEQhtlgrrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data saving for submission\n",
        "\n",
        "directory_path_test_lr = \"/content/drive/MyDrive/Colab_Notebooks/Hyundai_Daycon/test/lr\"\n",
        "test_image_files = [os.path.join(directory_path_test_lr, fname) for fname in os.listdir(directory_path_test_lr) if fname.endswith('.png') or fname.endswith('.jpg')]\n",
        "\n",
        "def load_and_preprocess_image(img_path):\n",
        "    img = load_img(img_path, target_size=(256, 256))\n",
        "    img = img_to_array(img)\n",
        "    img = img / 255.0\n",
        "    img_filter = tf.py_function(func=selective_median_filter, inp=[img], Tout=tf.float32)\n",
        "    return img_filter\n",
        "\n",
        "save_directory = \"/content/drive/MyDrive/Colab_Notebooks/Hyundai_Daycon/test_hr/231015_1\"\n",
        "\n",
        "for test_img_path in test_image_files:\n",
        "    test_img = load_and_preprocess_image(test_img_path)\n",
        "    test_img_input = tf.expand_dims(test_img, 0)\n",
        "\n",
        "    sr_img = model.predict(test_img_input)\n",
        "    sr_img = tf.squeeze(sr_img).numpy()\n",
        "    sr_img = np.clip(sr_img * 255, 0, 255).astype(np.uint8)  #De-normalizing\n",
        "    # RGB to BGR\n",
        "    sr_img_bgr = cv2.cvtColor(sr_img, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Use the same filename from input\n",
        "    filename = os.path.join(save_directory, os.path.basename(test_img_path))\n",
        "\n",
        "    cv2.imwrite(filename, sr_img_bgr)\n"
      ],
      "metadata": {
        "id": "aC9dSvM9sszz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}